{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目录\n",
    "\n",
    "- [0.书本概览](#0.书本概览)\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Multi-armed Bandits](#2.-Multi-armed-Bandits)\n",
    "- [3. Finite Markov DecisionProcesses](#3.-Finite-Markov-DecisionProcesses)\n",
    "- [4. Dynamic Programming](#4.-Dynamic-Programming)\n",
    "- [5. Monte Carlo Methods](#5.-Monte-Carlo-Methods)\n",
    "- [6. Temporal-Difference Learning](#6.-Temporal-Difference-Learning)\n",
    "- [7. n-step Bootstrapping](#7.-n-step-Bootstrapping)\n",
    "- [8. Planning and Learning with Tabular Methods](#8.-Planning-and-Learning-with-Tabular-Methods)\n",
    "- [9. On-policy Prediction with Approximation](#9.-On-policy-Prediction-with-Approximation)\n",
    "- [10. On-policy Control with Approximation](#10.-On-policy-Control-with-Approximation)\n",
    "- [11. Off-policy Methods with Approximation](#11.-Off-policy-Methods-with-Approximation)\n",
    "- [12. Eligibility Traces](#12.-Eligibility-Traces)\n",
    "- [13. Policy Gradient Methods](#13.-Policy-Gradient-Methods)\n",
    "- [14. Psychology](#14.-Psychology)\n",
    "- [15. Neuroscience](#15.-Neuroscience)\n",
    "- [16. Applications and Case Studies](#16.-Applications-and-Case-Studies)\n",
    "- [17. Frontiers](#17.-Frontiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.书本概览\n",
    "\n",
    "[Reinforcement Learning](https://book.douban.com/subject/2866455/)\n",
    "\n",
    "作者: Richard S. Sutton / Andrew G. Barto \n",
    "\n",
    "\n",
    "### 重要的符号表示\n",
    "- $q^∗(a)$：动作a对应的真实价值（价值期望），true value (expected reward) of action a\n",
    "- $Q_t(a)$：动作a在t时刻的估计价值，estimate at time t of $q^∗(a)$\n",
    "- $N_t(a)$：在t时刻之前，动作a已经被选择的次数，number of times action a has been selected up prior to time t\n",
    "- $ε$: ε贪婪策略　采取随机动作的概率，probability of taking a random action in an ε-greedy policy\n",
    "- $H_t(a)$：在t时刻选择动作a的偏好程度，learned preference for selecting action a at time t\n",
    "- $π_t(a)$：在t时刻选择动作a的概率，probability of selecting action a at time t\n",
    "- $R_t$：给定策略$π_t$，在t时刻的期望奖励，estimate at time t of the expected reward given$π_t$\n",
    "- $G_t$: 在ｔ时刻的回报，return following time t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 什么是强化学习\n",
    "强化学习是“学习该如何去做”（learning what to do），即学习如何从**某个状态映射到某个行为，来最大化某个数值的奖励信号**。\n",
    "\n",
    "### 强化学习的特征\n",
    "强化学习两个最重要的特征：\n",
    "- **试错（trial-and-error search ）**：agent需要不断尝试，通过reward的反馈学习策略。\n",
    "- **延迟奖励（delayed reward） **：某个时刻的action可能会对后面时刻的reward有影响（深远影响）。\n",
    "\n",
    "### Exploit vs Explore\n",
    "**exploit** 代表利用已有的信息去获得最大的奖励，**explore** 代表去探索一些没有尝试过的行为，去获得可能的更大的奖励。\n",
    "\n",
    "### 强化学习的几个要素\n",
    "- **policy**: 状态到行为的映射，定义agent在某个状态下应该如何采取行为，state $\\rightarrow$ action。\n",
    "- **reward function**: 在某个状态下，agent能够收到的**即时反馈**。\n",
    "- **value function**: 衡量在某个状态下，能够获得的**长期反馈**。\n",
    "- **modle (of the environment，可选的)**: 模型用来模拟环境的行为，**给定一个状态和动作，模型可以预测下一个状态和奖励**。\n",
    "   \n",
    "### RL vs Evolutionary Methods\n",
    "- Evolutionary Methods（遗传算法，具体可以回顾[之前的博客](https://applenob.github.io/ga.html)），直接在policy空间中搜索，并计算最后的得分。通过一代代的进化来找寻最优policy。\n",
    "- 遗传算法忽略了policy实际上是state到action的映射，它不关注agent和环境的互动，只看最终结果。\n",
    "\n",
    "### 局限性\n",
    "强化学习非常依赖**状态（state）**的概念。state既是策略函数和价值函数的输入，又是环境模型（model）的输入和输出。\n",
    "\n",
    "### Tic-Tac-Toe（井字棋）\n",
    "- ![](https://github.com/applenob/rl_learn/raw/master/res/ttt.png)\n",
    "- 一个简单的应用强化学习的例子。\n",
    "- 定义policy：任何一种局面下，该如何落子。\n",
    "- 遗传算法解法：试很多种policy，找到最终胜利的几种，然后结合，更新。\n",
    "- 强化学习解法：\n",
    "    - 1.建立一张表格，state_num × 1，代表每个state下，获胜的概率，这个表格就是所谓的**value function**，即状态到价值的映射。\n",
    "    - 2.跟对手下很多局。每次落子的时候，依据是在某个state下，选择所有可能的后继state中，获胜概率最大的（value最大的）。这种方法即贪婪法（Exploit）。偶尔我们也随机选择一些其他的state（Explore）。\n",
    "    - 3.“back up”后继state的v到当前state上。$V(s)\\leftarrow V(s)+\\alpha[V(s')-V(s)]$，这就是所谓的**差分学习（temporal-difference learning）**，这么叫是因为$V(s')-V(s)$是两个时间点上的两次估计的差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
